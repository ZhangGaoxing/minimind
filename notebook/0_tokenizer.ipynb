{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 0-Tokenizer\n",
    "\n",
    "Tokenizer（分词器）在 NLP 领域扮演着基础且关键的作用，它将文本分割成单词或子词并转化为数组编号，为模型提供可处理的输入，在文本预处理、语义理解及适配不同语言和任务等方面奠定基础，是连接自然语言文本与计算机可处理数据的重要桥梁."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 子词分词算法\n",
    "\n",
    "常见的子词分词算法有三种：\n",
    "\n",
    "1. 字节对编码（Byte Pair Encoding，BPE）\n",
    "2. WordPiece\n",
    "3. Unigram\n",
    "\n",
    "### BPE\n",
    "\n",
    "BPE 是一种简单的数据压缩技术，它会迭代地替换序列中最频繁出现的字节对。BPE 依赖一个预分词器，该预分词器会将训练数据分割成单词（在本项目中，我们使用按空格分词的方法作为预分词方法）.\n",
    "\n",
    "在预分词之后，会创建一组唯一的单词，并确定它们在数据中的出现频率。接下来，BPE 会创建一个基础词表，该词表包含预分词器最初生成的数据中所有唯一单词的符号。然后，会将这对符号从词表中移除，新形成的符号将加入词表。在迭代过程中，BPE 算法会合并频繁出现的符号对.\n",
    "\n",
    "给定词表的大小，BPE（字节对编码）算法最终会合并出现频率最高的符号对，直到收敛到该大小.\n",
    "\n",
    "### WordPiece\n",
    "\n",
    "WordPiece 算法与 BPE 非常相似。WordPiece 首先将词表初始化为包含训练数据中出现的每个字符，然后逐步学习给定数量的合并规则. 与 BPE 不同的是，WordPiece 并不选择最频繁出现的符号对，而是选择那个加入词表后能使训练数据出现的可能性最大化的符号对.\n",
    "\n",
    "### Unigram\n",
    "\n",
    "Unigram 算法将其基础词表初始化为大量的符号，然后逐步削减每个符号，以获得一个更小的词表。它会在训练数据上定义一个对数似然损失，以此来确定是否从词表中移除某个符号."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练一个最简单的分词器\n",
    "\n",
    "在本节中，我们将学习基于 transformers 库来训练你自己的分词器."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化\n",
    "\n",
    "首先，我们应该初始化我们的分词器，并确定选择哪种方法。我们将使用字节对编码（BPE）算法."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,  # 解码器\n",
    "    models,  # 模型\n",
    "    normalizers,  # 正则化器\n",
    "    pre_tokenizers,  # 预分词器\n",
    "    processors,  # 处理器\n",
    "    trainers,  # 训练器\n",
    "    Tokenizer,  # 分词器类\n",
    ")\n",
    "\n",
    "# 初始化一个基于 BPE（字节对编码）模型的分词器\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# 设置预分词器为 ByteLevel，确保分词器能够处理字节级别的分词\n",
    "# add_prefix_space=False 表示不在每个单词前添加空格\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义特殊标记\n",
    "\n",
    "数据集中存在一些我们不希望被分词的特殊标记，我们会将这些标记定义为特殊标记，并将它们传递给分词器训练器，以防止出现错误的分词情况."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义特殊标记列表，这些标记在分词过程中需要被特殊处理\n",
    "special_tokens = [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\"]\n",
    "\n",
    "# 初始化 BPE 训练器\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=6400,  # 设置词汇表大小为 6400\n",
    "    special_tokens=special_tokens,  # 确保特殊标记被包含在词汇表中\n",
    "    show_progress=True,  # 显示训练进度\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # 初始化字母表为字节级别的字母表\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从文件中读取数据\n",
    "\n",
    "在本次实验中，我们使用 JSON Lines（jsonl）格式来存储 Tokenizer 训练数据，分词器内置的训练函数要求训练数据以迭代器的形式传入，因此，我们首先获取一个数据读取的生成器."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_texts_from_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            yield data['text']\n",
    "\n",
    "data_path = './toydataset/tokenizer_data.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1: <|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等。\n",
      "文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "texts = read_texts_from_jsonl(data_path)\n",
    "print(f'Row 1: {next(texts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练!\n",
    "\n",
    "我们使用分词器的内置函数 `tokenizer.train_from_iterator` 来训练分词器."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用分词器的内置函数从迭代器中训练分词器\n",
    "tokenizer.train_from_iterator(texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置解码器为 ByteLevel 解码器\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，检查一下特殊标记是否得到了妥善处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保特殊标记的 ID 与预期一致\n",
    "assert tokenizer.token_to_id(\"<|endoftext|>\") == 0\n",
    "assert tokenizer.token_to_id(\"<|im_start|>\") == 1\n",
    "assert tokenizer.token_to_id(\"<|im_end|>\") == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将训练好的分词器保存到磁盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/toy_tokenizer\\\\vocab.json', './model/toy_tokenizer\\\\merges.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "tokenizer_dir = \"./model/toy_tokenizer\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\"))\n",
    "tokenizer.model.save(tokenizer_dir) # generate vocab.json & merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动创建一份配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"add_bos_token\": False,\n",
    "    \"add_eos_token\": False,\n",
    "    \"add_prefix_space\": False,\n",
    "    \"added_tokens_decoder\": {\n",
    "        \"0\": {\n",
    "            \"content\": \"<|endoftext|>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        },\n",
    "        \"1\": {\n",
    "            \"content\": \"<|im_start|>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"content\": \"<|im_end|>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        }\n",
    "    },\n",
    "    \"additional_special_tokens\": [],\n",
    "    \"bos_token\": \"<|im_start|>\",\n",
    "    \"clean_up_tokenization_spaces\": False,\n",
    "    \"eos_token\": \"<|im_end|>\",\n",
    "    \"legacy\": True,\n",
    "    \"model_max_length\": 32768,\n",
    "    \"pad_token\": \"<|endoftext|>\",\n",
    "    \"sp_model_kwargs\": {},\n",
    "    \"spaces_between_special_tokens\": False,\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"unk_token\": \"<|endoftext|>\",\n",
    "    \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{{ '<|im_start|>system\\\\n' + system_message + '<|im_end|>\\\\n' }}{% else %}{{ '<|im_start|>system\\\\nYou are a helpful assistant<|im_end|>\\\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\\\n' + content + '<|im_end|>\\\\n<|im_start|>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\\\n' }}{% endif %}{% endfor %}\"\n",
    "}\n",
    "\n",
    "with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenizer training completed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经训练了一个简单的分词器，并将其进行保存，接下来，我们试着加载它，并使用其帮助我们对文本进行编解码."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本：[{'role': 'system', 'content': '你是一个优秀的聊天机器人，总是给我正确的回应！'}, {'role': 'user', 'content': '你来自哪里？'}, {'role': 'assistant', 'content': '我来自地球'}]\n",
      "修改文本：<|im_start|>system\n",
      "你是一个优秀的聊天机器人，总是给我正确的回应！<|im_end|>\n",
      "<|im_start|>user\n",
      "你来自哪里？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "我来自地球<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 从保存的分词器目录加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/toy_tokenizer\")\n",
    "\n",
    "# 定义一组消息，包括系统消息、用户消息和助手消息\n",
    "msg = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个优秀的聊天机器人，总是给我正确的回应！\"},\n",
    "    {\"role\": \"user\", \"content\": '你来自哪里？'},\n",
    "    {\"role\": \"assistant\", \"content\": '我来自地球'}\n",
    "]\n",
    "\n",
    "# 使用分词器的聊天模板功能对消息进行格式化\n",
    "new_msg = tokenizer.apply_chat_template(\n",
    "    msg,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "# 打印原始消息和格式化后的消息\n",
    "print(f'原始文本：{msg}')\n",
    "print(f'修改文本：{new_msg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer实际词表长度： 1898\n",
      "encoder长度： 65\n"
     ]
    }
   ],
   "source": [
    "# 获取实际词汇表长度（包括特殊符号）\n",
    "actual_vocab_size = len(tokenizer)\n",
    "print('tokenizer实际词表长度：', actual_vocab_size)\n",
    "model_inputs = tokenizer(new_msg)\n",
    "print('encoder长度：', len(model_inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看分词结果：\n",
      "{'input_ids': [1, 85, 91, 277, 71, 79, 201, 329, 303, 328, 1528, 265, 683, 1042, 550, 854, 327, 269, 1311, 834, 582, 678, 265, 434, 324, 245, 425, 2, 201, 1, 87, 85, 71, 84, 201, 329, 997, 101, 444, 383, 422, 237, 291, 2, 201, 1, 67, 85, 85, 75, 277, 67, 80, 86, 201, 284, 997, 101, 444, 438, 166, 241, 228, 2, 201], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(new_msg)\n",
    "print(f'查看分词结果：\\n{model_inputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看解码结果：\n",
      "<|im_start|>system\n",
      "你是一个优秀的聊天机器人，总是给我正确的回应！<|im_end|>\n",
      "<|im_start|>user\n",
      "你来自哪里？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "我来自地球<|im_end|>\n",
      "\n",
      "decoder和原始文本是否一致： True\n"
     ]
    }
   ],
   "source": [
    "input_ids = model_inputs['input_ids']\n",
    "response = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "print(f'查看解码结果：\\n{response}')\n",
    "print('decoder和原始文本是否一致：', response == new_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "\n",
    "- [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/zh-CN/chapter2/4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
