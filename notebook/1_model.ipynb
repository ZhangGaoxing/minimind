{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f359c4f9-9082-4843-8874-4e7c6f3c7404",
   "metadata": {},
   "source": [
    "# 1-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a93fb-2d09-4af4-9782-578357927842",
   "metadata": {},
   "source": [
    "在当前人工智能领域, 主流大模型从架构上大致可分为稠密（Dense）模型和混合专家（Mixture of Expert, MoE）模型 。稠密模型中所有参数在每次计算时都会参与运算；混合专家模型则将不同的 “专家” 模块组合, 根据输入选择合适的专家处理, 能在保证效果的同时减少计算量和参数量.\n",
    "\n",
    "MiniMind 系列模型在 Llama 3.1 的基础上设计, 基于经典的 Transformer Deocder-Only 架构，在这一板块, 我们将围绕 MiniMind 系列模型的源代码展开学习."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e2248-7b71-442f-96a0-01a5cb4a1171",
   "metadata": {},
   "source": [
    "## MiniMind Dense Model\n",
    "\n",
    "作者提供了对其 MiniMind Dense Model 模型结构的可视化：\n",
    "\n",
    "![image](../images/LLM-structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd2aa37-3460-4a64-8285-f888d62b99d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import PretrainedConfig\n",
    "from transformers.activations import ACT2FN\n",
    "from typing import Optional, Tuple, List, Union\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "class MiniMindConfig(PretrainedConfig):\n",
    "    model_type = \"minimind\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dropout: float = 0.0,\n",
    "            bos_token_id: int = 1,\n",
    "            eos_token_id: int = 2,\n",
    "            hidden_act: str = 'silu',\n",
    "            hidden_size: int = 512,\n",
    "            intermediate_size: int = None,\n",
    "            max_position_embeddings: int = 32768,\n",
    "            num_attention_heads: int = 8,\n",
    "            num_hidden_layers: int = 8,\n",
    "            num_key_value_heads: int = 2,\n",
    "            vocab_size: int = 6400,\n",
    "            rms_norm_eps: float = 1e-05,\n",
    "            rope_theta: int = 1000000.0,\n",
    "            flash_attn: bool = True,\n",
    "            ####################################################\n",
    "            # Here are the specific configurations of MOE\n",
    "            # When use_moe is false, the following is invalid\n",
    "            ####################################################\n",
    "            use_moe: bool = False,\n",
    "            num_experts_per_tok: int = 2,\n",
    "            n_routed_experts: int = 4,\n",
    "            n_shared_experts: int = 1,\n",
    "            scoring_func: str = 'softmax',\n",
    "            aux_loss_alpha: float = 0.1,\n",
    "            seq_aux: bool = True,\n",
    "            norm_topk_prob: bool = True,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout = dropout\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.hidden_act = hidden_act\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.rope_theta = rope_theta\n",
    "        self.flash_attn = flash_attn\n",
    "        ####################################################\n",
    "        # Here are the specific configurations of MOE\n",
    "        # When use_moe is false, the following is invalid\n",
    "        ####################################################\n",
    "        self.use_moe = use_moe\n",
    "        self.num_experts_per_tok = num_experts_per_tok  # 每个token选择的专家数量\n",
    "        self.n_routed_experts = n_routed_experts  # 总的专家数量\n",
    "        self.n_shared_experts = n_shared_experts  # 共享专家\n",
    "        self.scoring_func = scoring_func  # 评分函数，默认为'softmax'\n",
    "        self.aux_loss_alpha = aux_loss_alpha  # 辅助损失的alpha参数\n",
    "        self.seq_aux = seq_aux  # 是否在序列级别上计算辅助损失\n",
    "        self.norm_topk_prob = norm_topk_prob  # 是否标准化top-k概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb3ad8-5143-45ff-af16-8dcc0cc2cf53",
   "metadata": {},
   "source": [
    "### 均方根层归一化 (Root Mean Square Layer Normalization, RMSNorm)\n",
    "\n",
    "RMSNorm 是对 LayerNorm 的一个改进,  没有做 re-center 操作（移除了均值项）, 可以看作 LayerNorm 在均值为零时的特例, 使用平方根均值归一化降低噪声影响。\n",
    "\n",
    "- **Layer Norm**\n",
    "\n",
    "$$y = \\frac{x-E(x)}{\\sqrt{Var(x) + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "假设输入张量形状为 (batch_size,  sequence_length,  embedding_dim), 层归一化对 embedding_dim 维度进行归一化操作, 其中,  $\\epsilon$ 是一个超参数, 用于防止分母为零导致结果上溢,  $\\gamma$,  $\\beta$ 均为可学习参数。\n",
    "\n",
    "- **RMS Norm**\n",
    "\n",
    "$$a_i=\\frac{a_i}{RMS(a) + \\epsilon} * \\gamma,  \\quad where \\quad RMS(a) = \\sqrt{\\frac{1}{n}\\sum^n_{i=1}a^2_i}.$$\n",
    "\n",
    "假设输入张量形状为 (batch_size,  sequence_length,  embedding_dim), RMS Norm 对 embedding_dim 维度进行归一化,其中,  其中,  $\\epsilon$ 是一个超参数, 用于防止分母为零导致结果上溢, $\\gamma$ 为可学习参数.\n",
    "\n",
    "不难发现, 当均值为零时, Layer Norm 退化为 RMS Norm. 这是因为 RMS Norm 在 Layer Norm 的基础上舍弃了中心化操作, 仅用缩放进行归一化, 其不改变数据原本的分布, 有利于激活函数输出的稳定."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c52df1c5-1047-48d8-aa00-c0b576a87632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps  # 防止分母等于零导致结果上溢\n",
    "        self.weight = nn.Parameter(torch.ones(dim)) # 可学习参数\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6207c186-9816-41fd-872d-d5116257b274",
   "metadata": {},
   "source": [
    "### Rotary Position Embedding, RoPE\n",
    "\n",
    "旋转位置编码是一种能将相对位置信息集成到 self-attention 中, 进而提升 transformer 架构性能的位置编码方式, 和绝对位置编码相比, RoPE 具有很好的外推性, 是目前的主流位置编码方式.\n",
    "\n",
    "外推性的解释, 通俗来说就是训练的时候限制了 512 的上下文长度，那么推理时如果面对超过该长度的文本，LLM 可能无法正确处理.\n",
    "\n",
    "- **绝对位置编码**\n",
    "\n",
    "绝对位置编码是早期 Transformer 架构采用的绝对位置编码方案，及那个每个位置映射为固定的向量表示.\n",
    "\n",
    "$$f_{t:t\\in\\{q,k,v\\}}(\\boldsymbol{x}_i,i)=\\boldsymbol{W}_{t:t\\in\\{q,k,v\\}}(\\boldsymbol{x}_i+\\boldsymbol{p}_i)$$\n",
    "\n",
    "其中编码向量 $p_i$ 的计算使用如下公式：\n",
    "\n",
    "$$\\boldsymbol{p}_{i,2t}=\\sin\\left(k/1000^{2t/d}\\right), \\boldsymbol{p}_{i,2t+1}=\\cos\\left(k/1000^{2t/d}\\right)$$\n",
    "\n",
    "正如其名，绝对位置编码只考虑了输入序列中的绝对位置关系，对于 token 之间的相对信息则没有纳入考虑.\n",
    "\n",
    "- **旋转位置编码**\n",
    "\n",
    "假定 query 和 key 的内积操作可以被函数 g 表示，该函数 g 的输入是词嵌入向量 $x_m, x_n$ 和它们之间的相对位置 $m-n$:\n",
    "\n",
    "$$<f_q(x_m ,m), f_k(x_n, n)>=g(x_m, x_n, m, n)$$\n",
    "\n",
    "旋转位置编码就是找到一个使上式成立的位置编码方式. \n",
    "\n",
    "出于认识的目的，我们省略复杂的数学推导，直接看 RoPE 的的结论：\n",
    "\n",
    "存在这样一个正交矩阵：\n",
    "\n",
    "$$\\boldsymbol{R}_{\\Theta,m}^d=\\underbrace{\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\end{pmatrix}}_{\\boldsymbol{W}_m}$$\n",
    "\n",
    "其中，$\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}$\n",
    "\n",
    "我们可以将 query 和 key 的内积操作转换为与原始向量 $x$ 相关的以下等价形式：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{q}_m^\\mathbf{T}\\boldsymbol{k}_n=\\left(\\boldsymbol{R}_{\\Theta,m}^d\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)^\\mathbf{T}\\left(\\boldsymbol{R}_{\\Theta,n}^d\\boldsymbol{W}_k\\boldsymbol{x}_n\\right)=\\boldsymbol{x}_m^\\mathbf{T}\\boldsymbol{W}_q\\boldsymbol{R}_{\\Theta,n-m}^d\\boldsymbol{W}_k\\boldsymbol{x}_n\n",
    "$$\n",
    "\n",
    "其中， $\\boldsymbol{R}_{\\Theta,n-m}^d=\\left(\\boldsymbol{R}_{\\Theta,m}^d\\right)^\\mathbf{T}\\boldsymbol{R}_{\\Theta,n}^d$.\n",
    "\n",
    "由于 $\\boldsymbol{R}_{\\Theta,m}^d$ 的稀疏性，直接使用矩阵乘法会浪费算力，因此代码中采用下述方式实现：\n",
    "\n",
    "$$\\boldsymbol{R}_{\\Theta,m}^{d}\\boldsymbol{x}=\\begin{pmatrix}x_{0}\\\\x_{1}\\\\x_{2}\\\\x_{3}\\\\\\vdots\\\\x_{d-2}\\\\x_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_{0}\\\\\\cos m\\theta_{0}\\\\\\cos m\\theta_{1}\\\\\\cos m\\theta_{1}\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}+\\begin{pmatrix}-x_{1}\\\\x_{0}\\\\-x_{3}\\\\x_{2}\\\\\\vdots\\\\-x_{d-1}\\\\x_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_{0}\\\\\\sin m\\theta_{0}\\\\\\sin m\\theta_{1}\\\\\\sin m\\theta_{1}\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae4194-3738-44a7-9c33-bbf964067f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):\n",
    "    \"\"\"\n",
    "    预计算旋转位置编码的频率和相位。\n",
    "\n",
    "    参数:\n",
    "    - dim (int): 嵌入维度。\n",
    "    - end (int): 序列长度的最大值，默认为 32 * 1024。\n",
    "    - theta (float): 控制频率范围的参数，默认为 1e6。\n",
    "\n",
    "    返回:\n",
    "    - freqs_cos (torch.Tensor): 余弦频率张量，形状为 (end, dim)。\n",
    "    - freqs_sin (torch.Tensor): 正弦频率张量，形状为 (end, dim)。\n",
    "    \"\"\"\n",
    "    # 计算频率\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    # 生成时间步\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    # 计算频率矩阵\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    # 计算余弦和正弦频率\n",
    "    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)\n",
    "    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)\n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"\n",
    "    应用旋转位置编码到 Query 和 Key 向量。\n",
    "\n",
    "    参数:\n",
    "    - q (torch.Tensor): Query 向量，形状为 (..., head_dim)。\n",
    "    - k (torch.Tensor): Key 向量，形状为 (..., head_dim)。\n",
    "    - cos (torch.Tensor): 预计算的余弦频率张量。\n",
    "    - sin (torch.Tensor): 预计算的正弦频率张量。\n",
    "    - position_ids (Optional[torch.Tensor]): 位置索引，默认为 None。\n",
    "    - unsqueeze_dim (int): 在余弦和正弦频率张量中扩展维度的索引，默认为 1。\n",
    "\n",
    "    返回:\n",
    "    - q_embed (torch.Tensor): 应用旋转位置编码后的 Query 向量。\n",
    "    - k_embed (torch.Tensor): 应用旋转位置编码后的 Key 向量。\n",
    "    \"\"\"\n",
    "    def rotate_half(x):\n",
    "        \"\"\"\n",
    "        对张量进行旋转操作，将后一半的值取负并与前一半交换位置。\n",
    "\n",
    "        参数:\n",
    "        - x (torch.Tensor): 输入张量。\n",
    "\n",
    "        返回:\n",
    "        - torch.Tensor: 旋转后的张量。\n",
    "        \"\"\"\n",
    "        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), dim=-1)\n",
    "\n",
    "    # 对 Query 和 Key 应用旋转位置编码\n",
    "    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))\n",
    "    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff57879-1f39-404d-9501-3c99e2e67c41",
   "metadata": {},
   "source": [
    "我们知道，RoPE 是在 Attention 阶段生成 Query 和 Key 向量后，对这两个向量进行位置编码的.\n",
    "\n",
    "对于 MiniMindLM，嵌入维度为 512，注意力头数量为 8，故每一个注意力头的维度应该为 512 / 8 = 64.\n",
    "\n",
    "我们使用固定形状的张量代表 Query 和 Key 向量，对 RoPE 的应用过程进行观察."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8376c4-5b00-4c72-9f23-912478db581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xq,  xk = torch.randn((2,  16,  4,  64)), torch.randn((2,  16,  4,  64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47eeb0-874e-4df3-bf91-5f79109af8ca",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "注意力机制（Attention Mechanism）是Transformer架构的核心组件，能够有效捕捉长序列内各元素间的依赖关系. 该机制通过计算输入序列中不同位置元素间的注意力得分，对其重要性进行精准建模，使模型在处理信息时能够聚焦于关键部分，从而显著提升对长序列数据的理解与处理能力.\n",
    "\n",
    "在 MiniMindLM 模型中, Attention Block 包含以下机制和模块:\n",
    "\n",
    "1. GQA\n",
    "2. KV Cache\n",
    "5. SwiGLU\n",
    "\n",
    "- **GQA**\n",
    "\n",
    "分组查询注意力 (Group Querey Attention, GQA) 是对多头自注意力机制的扩展, 通过提供计算效率和模型表达能力的灵活权衡, 实现了查询头的分组.\n",
    "\n",
    "具体来说, GQA 中将 h 个查询头分为 G 组, 每组 包含 h / G 个查询头，并共享一个公共的键和值.\n",
    "\n",
    "![img](./images/gqa.png)\n",
    "\n",
    "GQA 相比传统的 MHA， 减少了键和值的数量，降低了计算量和内存开销，提高了推理速度.\n",
    "\n",
    "- **KV Cache**\n",
    "\n",
    "在语言模型生成文本的过程中，每生成一个新的 token，模型都需要计算注意力得分，以确定当前位置与之前所有位置的相关性.\n",
    "\n",
    "比如以下内容：\n",
    "\n",
    "1. *seq = \\[tok1]:*\n",
    "\n",
    "   attn_11 = softmax(Q1 * K1.T / sqrt(dim)) * V1\n",
    "   \n",
    "3. *seq = \\[tok1, tok2]:*\n",
    "\n",
    "   attn_11 = softmax(Q1 * K1.T / sqrt(dim)) * V1, attn_12 = 0 (masked)\n",
    "   \n",
    "   attn_21 = softmax(Q2 * K1.T / sqrt(dim)) * V1, attn_22 = softmax(Q2 * K2.T / sqrt(dim)) * V2\n",
    "4. *seq = \\[tok1, tok2, tok3]:*\n",
    "\n",
    "   attn_11 = softmax(Q1 * K1.T / sqrt(dim)) * V1, attn_12 = 0 (masked), attn_13 = 0 (masked)\n",
    "   \n",
    "   attn_21 = softmax(Q2 * K1.T / sqrt(dim)) * V1, attn_22 = softmax(Q2 * K2.T / sqrt(dim)) * V2, attn_23 = 0 (masked)\n",
    "   \n",
    "   attn_31 = softmax(Q3 * K1.T / sqrt(dim)) * V1, attn_32 = softmax(Q3 * K2.T / sqrt(dim)) * V2, attn_33 = softmax(Q3 * K3.T / sqrt(dim)) * V3\n",
    "5.  ··· ···\n",
    "\n",
    "不难发现，大模型生成一个 token 后的注意力计算中，总会用到 token 序列的历史 KV 值，导致重复计算，KV Cache 的设计正是为了通过缓存历史 KV 值，节省计算开销.\n",
    "\n",
    "KV Cache 能够有效压缩大模型推理时的显存占用.\n",
    "\n",
    "- **SwiGLU**\n",
    "\n",
    "SwiGLU 是一种在深度学习中用于神经网络架构的激活函数变体：\n",
    "\n",
    "$$\\text{SwiGLU}(x, W, V, b, c)=\\text{Swish}_1(xW+b)\\otimes(xV+c)$$\n",
    "\n",
    "与传统的 ReLU 激活函数相比，SwiGLU 具有更好的平滑性和非线性表达能力，由于其门控机制，在处理信息筛选和流动方面有独特的优势."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61394db-5afa-4421-ba77-30a9c50f0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    重复键值张量以适应注意力头的数量。\n",
    "\n",
    "    参数:\n",
    "    - x (torch.Tensor): 输入张量，形状为 (batch_size, seq_len, num_key_value_heads, head_dim)。\n",
    "    - n_rep (int): 每个键值头需要重复的次数。\n",
    "\n",
    "    返回:\n",
    "    - torch.Tensor: 重复后的张量，形状为 (batch_size, seq_len, num_key_value_heads * n_rep, head_dim)。\n",
    "    \"\"\"\n",
    "    bs, slen, num_key_value_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x  # 如果不需要重复，直接返回原张量\n",
    "    return (\n",
    "        x[:, :, :, None, :]  # 在 num_key_value_heads 维度后插入一个新维度\n",
    "        .expand(bs, slen, num_key_value_heads, n_rep, head_dim)  # 扩展新维度以重复 n_rep 次\n",
    "        .reshape(bs, slen, num_key_value_heads * n_rep, head_dim)  # 重塑张量形状以合并重复维度\n",
    "    )\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    自定义注意力模块，支持旋转位置编码和 Flash Attention。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, args: MiniMindConfig):\n",
    "        super().__init__()\n",
    "        # 初始化注意力模块的参数\n",
    "        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads\n",
    "        assert args.num_attention_heads % self.num_key_value_heads == 0  # 确保注意力头数量可以被键值头数量整除\n",
    "        self.n_local_heads = args.num_attention_heads  # 本地注意力头数量\n",
    "        self.n_local_kv_heads = self.num_key_value_heads  # 本地键值头数量\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads  # 每个键值头需要重复的次数\n",
    "        self.head_dim = args.hidden_size // args.num_attention_heads  # 每个注意力头的维度\n",
    "        # 定义线性投影层\n",
    "        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)\n",
    "        # 定义 Dropout 层\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "        self.dropout = args.dropout\n",
    "        # 检查是否支持 Flash Attention\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                position_embeddings: Tuple[torch.Tensor, torch.Tensor],  # 接收预计算的 cos 和 sin\n",
    "                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # 历史键值缓存\n",
    "                use_cache=False,  # 是否使用缓存\n",
    "                attention_mask: Optional[torch.Tensor] = None):  # 注意力掩码\n",
    "        bsz, seq_len, _ = x.shape\n",
    "        # 计算 Query, Key, Value\n",
    "        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        # 应用旋转位置编码\n",
    "        cos, sin = position_embeddings\n",
    "        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])\n",
    "\n",
    "        # 处理 KV 缓存\n",
    "        if past_key_value is not None:\n",
    "            xk = torch.cat([past_key_value[0], xk], dim=1)  # 拼接历史键值\n",
    "            xv = torch.cat([past_key_value[1], xv], dim=1)\n",
    "        past_kv = (xk, xv) if use_cache else None  # 更新缓存\n",
    "\n",
    "        # 重复键值以适应注意力头数量\n",
    "        xq, xk, xv = (\n",
    "            xq.transpose(1, 2),\n",
    "            repeat_kv(xk, self.n_rep).transpose(1, 2),\n",
    "            repeat_kv(xv, self.n_rep).transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        # 使用 Flash Attention 或传统注意力计算\n",
    "        if self.flash and seq_len != 1:\n",
    "            dropout_p = self.dropout if self.training else 0.0\n",
    "            attn_mask = None\n",
    "            if attention_mask is not None:\n",
    "                attn_mask = attention_mask.view(bsz, 1, 1, -1).expand(bsz, self.n_local_heads, seq_len, -1)\n",
    "                attn_mask = attn_mask.bool() if attention_mask is not None else None\n",
    "\n",
    "            # 使用 Flash Attention\n",
    "            output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=True)\n",
    "        else:\n",
    "            # 传统注意力计算\n",
    "            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)  # 计算注意力得分\n",
    "            scores = scores + torch.triu(\n",
    "                torch.full((seq_len, seq_len), float(\"-inf\"), device=scores.device),\n",
    "                diagonal=1\n",
    "            ).unsqueeze(0).unsqueeze(0)  # 添加掩码\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9\n",
    "                scores = scores + extended_attention_mask\n",
    "\n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)  # 归一化注意力得分\n",
    "            scores = self.attn_dropout(scores)  # 应用 Dropout\n",
    "            output = scores @ xv  # 计算注意力输出\n",
    "\n",
    "        # 重塑输出形状并应用残差 Dropout\n",
    "        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)\n",
    "        output = self.resid_dropout(self.o_proj(output))\n",
    "        return output, past_kv  # 返回输出和更新后的缓存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420b47b-b38e-442a-84a2-e307f82584d3",
   "metadata": {},
   "source": [
    "同样的，我们假设一批 batch size = 4， seq len = 16 的 token 序列通过这个 Attention 块，在输入前，它的 input id 会被投影到 dim = 512 维."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1519451-3dc8-498e-96a7-db8b32a71af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Config，为了简单起见，我们设定 num_hidden_layers = 2\n",
    "LMConfig_Dense = MiniMindConfig(num_hidden_layers=2)\n",
    "# attn = Attention(LMConfig_Dense)\n",
    "# x = torch.randn((4,  16,  512)) # (batch size, seq len, embed dim)\n",
    "# pos_cis = precompute_pos_cis(64,  16) # (head dim, batch size) 其中 head dim = embed dim / num heads\n",
    "# output,  past_kv = attn(x,  pos_cis=pos_cis,  use_cache=True)\n",
    "# print(f'输入张量 x ：size = {x.shape}，RoPE 旋转角： size = {pos_cis.shape}')\n",
    "# print(f'输出 output: size = {output.shape},  kv_cache 基本信息：size_key = {past_kv[0].shape}, size_value = {past_kv[1].shape}')\n",
    "# del attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f1e81-36de-4e94-8ae4-69490f62d15f",
   "metadata": {},
   "source": [
    "### FeedForward Network\n",
    "\n",
    "前馈神经网络接收来自注意力机制层的输出结果，随后对该输出执行进一步的线性变换. 通过这种方式，网络能够深入挖掘并捕获更为复杂、抽象的特征."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a16f3ece-3b4e-42aa-9b88-1a7193dd2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config: MiniMindConfig):\n",
    "        super().__init__()\n",
    "        # 如果未指定中间层大小，则根据隐藏层大小计算默认值\n",
    "        if config.intermediate_size is None:\n",
    "            intermediate_size = int(config.hidden_size * 8 / 3)\n",
    "            # 将中间层大小调整为64的倍数\n",
    "            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)\n",
    "        \n",
    "        # 定义前馈网络的线性层\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)  # 输入到中间层的投影\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)  # 中间层到输出的投影\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)  # 输入到中间层的另一路投影\n",
    "        self.dropout = nn.Dropout(config.dropout)  # Dropout层\n",
    "        self.act_fn = ACT2FN[config.hidden_act]  # 激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播：激活函数作用于 gate_proj 的输出，并与 up_proj 的输出相乘，最后通过 down_proj 和 Dropout\n",
    "        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb2503-018c-4486-ae45-d9d0a192490a",
   "metadata": {},
   "source": [
    "设置输入 x 为 batch size = 4，seq len = 16 的 token 序列投影向量，观察 x 在 MiniMind Block 的前向传播过程."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe2abd9-0c40-44be-b64d-8b0caa28604b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给定输入 x: size = torch.Size([4, 16, 512]) 下的输出 output：size = torch.Size([4, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(LMConfig_Dense)\n",
    "x = torch.randn((4,  16,  512))\n",
    "output = ffn(x)\n",
    "print(f'给定输入 x: size = {x.shape} 下的输出 output：size = {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ae0c7e5-67e9-4dc4-9ea5-4ec20667c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ffn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe25d3b-2af9-4fe1-8eb0-7522324ec2c6",
   "metadata": {},
   "source": [
    "这就是 Transformer 结构的精妙之处，张量在模型内部进行了各种复杂的投影变形，但是输入输出的张量形状不发生改变!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2317829c-39b8-44d5-beb2-1b9727bc35c8",
   "metadata": {},
   "source": [
    "### MiniMind Block\n",
    "\n",
    "到目前为止, , 已经完成了 Attention Layer 和 FeedForward Layer 的构建, 所有必须的组件都已经具备, 我们着手构建一个 MiniMind Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6c224a4-7ba5-4cf8-9297-311c660f5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniMindBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, config: MiniMindConfig):\n",
    "        super().__init__()\n",
    "        # 初始化参数\n",
    "        self.num_attention_heads = config.num_attention_heads  # 注意力头数量\n",
    "        self.hidden_size = config.hidden_size  # 隐藏层大小\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads  # 每个注意力头的维度\n",
    "        self.self_attn = Attention(config)  # 自注意力模块\n",
    "\n",
    "        self.layer_id = layer_id  # 当前层的编号\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)  # 输入层归一化\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)  # 注意力后的归一化\n",
    "        self.mlp = FeedForward(config)  # 前馈神经网络\n",
    "\n",
    "    def forward(self, hidden_states, position_embeddings, past_key_value=None, use_cache=False, attention_mask=None):\n",
    "        # 残差连接\n",
    "        residual = hidden_states\n",
    "        # 自注意力计算\n",
    "        hidden_states, present_key_value = self.self_attn(\n",
    "            self.input_layernorm(hidden_states),  # 对输入进行归一化\n",
    "            position_embeddings,  # 位置编码\n",
    "            past_key_value,  # 历史键值缓存\n",
    "            use_cache,  # 是否使用缓存\n",
    "            attention_mask  # 注意力掩码\n",
    "        )\n",
    "        hidden_states += residual  # 添加残差连接\n",
    "        # 前馈神经网络计算并添加残差连接\n",
    "        hidden_states = hidden_states + self.mlp(self.post_attention_layernorm(hidden_states))\n",
    "        return hidden_states, present_key_value  # 返回更新后的隐藏状态和键值缓存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627c780-f07e-41c3-8b44-7a819e80a2a4",
   "metadata": {},
   "source": [
    "我们依然设置输入 x 为 batch size = 4，seq len = 16 的 token 序列投影向量，观察 x 在 MiniMind Block 的前向传播过程."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b4461-534c-4f6d-a00b-cdcfbd4d7bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出 output 信息: size = torch.Size([4, 16, 512])\n",
      "该 Block 维护的 KV Cache 信息：size_key =  torch.Size([4, 16, 2, 64]), size_value = torch.Size([4, 16, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "# miniblock = MiniMindBlock(1,  LMConfig_Dense)\n",
    "# x = torch.randn((4,  16,  512))\n",
    "# pos_cis = precompute_pos_cis(64,  16)\n",
    "# out,  past_kv = miniblock(x,  pos_cis,  use_cache=True)\n",
    "# print(f'输出 output 信息: size = {out.shape}\\n该 Block 维护的 KV Cache 信息：size_key =  {past_kv[0].shape}, size_value = {past_kv[1].shape}')\n",
    "# del miniblock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bb3b2e-4d1d-4721-9908-546f3c49dde3",
   "metadata": {},
   "source": [
    "### MiniMindLM (Dense)\n",
    "\n",
    "以 MiniMind Block 为基本组件, 我们对 MiniMindLM 进行最后组装！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7267245b-f13a-4bcf-9358-5fdd89106153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniMindModel(nn.Module):\n",
    "    def __init__(self, config: MiniMindConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vocab_size, self.num_hidden_layers = config.vocab_size, config.num_hidden_layers\n",
    "        # 定义词嵌入层\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        # 定义 Dropout 层\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        # 定义多个 MiniMindBlock 层\n",
    "        self.layers = nn.ModuleList([MiniMindBlock(l, config) for l in range(self.num_hidden_layers)])\n",
    "        # 定义 RMSNorm 层\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        # 预计算旋转位置编码的频率和相位\n",
    "        freqs_cos, freqs_sin = precompute_freqs_cis(dim=config.hidden_size // config.num_attention_heads,\n",
    "                                                    end=config.max_position_embeddings, theta=config.rope_theta)\n",
    "        # 注册为模型的缓冲区\n",
    "        self.register_buffer(\"freqs_cos\", freqs_cos, persistent=False)\n",
    "        self.register_buffer(\"freqs_sin\", freqs_sin, persistent=False)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: Optional[torch.Tensor] = None,  # 输入的 token ID\n",
    "                attention_mask: Optional[torch.Tensor] = None,  # 注意力掩码\n",
    "                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,  # 历史键值缓存\n",
    "                use_cache: bool = False,  # 是否使用缓存\n",
    "                **kwargs):\n",
    "        # 获取批次大小和序列长度\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        # 如果没有提供 past_key_values，则初始化为空\n",
    "        past_key_values = past_key_values or [None] * len(self.layers)\n",
    "        # 获取起始位置\n",
    "        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0\n",
    "\n",
    "        # 嵌入输入的 token 并应用 Dropout\n",
    "        hidden_states = self.dropout(self.embed_tokens(input_ids))\n",
    "\n",
    "        # 获取当前位置的旋转位置编码\n",
    "        position_embeddings = (\n",
    "            self.freqs_cos[start_pos:start_pos + seq_length],\n",
    "            self.freqs_sin[start_pos:start_pos + seq_length]\n",
    "        )\n",
    "\n",
    "        presents = []  # 用于存储每一层的键值缓存\n",
    "        for layer_idx, (layer, past_key_value) in enumerate(zip(self.layers, past_key_values)):\n",
    "            # 通过每一层的前向传播\n",
    "            hidden_states, present = layer(\n",
    "                hidden_states,\n",
    "                position_embeddings,  # 位置编码\n",
    "                past_key_value=past_key_value,  # 历史键值缓存\n",
    "                use_cache=use_cache,  # 是否使用缓存\n",
    "                attention_mask=attention_mask  # 注意力掩码\n",
    "            )\n",
    "            presents.append(present)  # 保存当前层的键值缓存\n",
    "\n",
    "        # 应用 RMSNorm 层\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # MoE 辅助损失，在 Dense 模型中不纳入考虑\n",
    "        aux_loss = 0\n",
    "\n",
    "        # 返回隐藏状态、键值缓存和辅助损失\n",
    "        return hidden_states, presents, aux_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee2ebc-f98c-49cc-b5fc-c6cd0056f4d9",
   "metadata": {},
   "source": [
    "接下来，我们设置一条长度为 4 的 token 序列，使用 MiniMindLM 对其执行一次前向传播，观察传播过程与返回值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b4d79-a3aa-4143-aa5c-2abce34bc96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "返回 logits：size = torch.Size([1, 4, 512]), 返回 aux_loss: 0,  返回 KV Cache List： len = 2\n"
     ]
    }
   ],
   "source": [
    "MiniMind_Dense = MiniMindModel(LMConfig_Dense)\n",
    "input_ids = torch.Tensor([1,  3,  5,  7]).long().reshape(1,  4)\n",
    "OUT = MiniMind_Dense(input_ids,  use_cache=True)\n",
    "print(f'返回 logits：size = {OUT[0].shape}, 返回 aux_loss: {OUT[2]},  返回 KV Cache List： len = {len(OUT[1])}')\n",
    "del MiniMind_Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e4ef5f-1358-4ffb-a150-61141ac3062f",
   "metadata": {},
   "source": [
    "## Minimind MoE Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ced18-db3a-478b-9578-9825634f3d66",
   "metadata": {},
   "source": [
    "作者提供了 MiniMind MoE Model 的可视化.\n",
    "\n",
    "![image](../images/LLM-structure-moe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82682a98-cae7-49c2-be2f-12dd1adb8deb",
   "metadata": {},
   "source": [
    "可以看到，Dense Model 和 MoE Model 的差异在于 FFN 层. MoE 模型将稠密连接的 FFN 层置换为 M x Expert 层，每次前向传播时只激活部分 Expert.\n",
    "\n",
    "其组成可以分为以下部分：\n",
    "\n",
    "- Experts: MoE 架构单元，每个专家本质上是一个独立的神经网络模块，负责处理特定类型或范围的数据.\n",
    "- Router： 控制信息流动，决定每次前向传播激活的 Experts 模块以及输入数据在这些模块的分配组合.\n",
    "\n",
    "在 MoE 网络中，为了平衡专家的重要性，我们需要关注路由，它是决定在特定时间选择哪些专家的重要组件.\n",
    "\n",
    "### 辅助损失\n",
    "\n",
    "为了让训练过程中实现专家的更均匀分布，辅助损失（又称负载均衡损失）被添加到网络的常规损失中，它增加了一个约束，迫使专家具有相等的重要性.\n",
    "\n",
    "假设有输入序列 \\[What is Mixture of Experts], *Prob* (·) 表示每一个 token 激活的专家概率分布:\n",
    "\n",
    "- **Step 1**：在整个批次中对每个专家的路由值进行求和.\n",
    "\n",
    "   $$Importance \\, per \\, token = Prob (What) + Prob (is) + Prob (Mixture) + Prob (of) + Prob (Experts)$$\n",
    "\n",
    "   这个指标反映了 batch 维度上每个专家的重要性分数.\n",
    "\n",
    "- **Step 2**: 计算变异系数\n",
    "\n",
    "   我们希望专家之间的重要性尽可能靠近，为了衡量专家得分之间的差异程度，引入变异系数指标（Coefficient Variation, CV）\n",
    "\n",
    "   $$Coeifficient \\, Variation (CV) = \\dfrac{standard \\, deviation (\\sigma)}{mean(\\mu)}$$\n",
    "\n",
    "   如果专家的重要性分数相似，变异系数会降到很低（这是我们期望的）\n",
    "\n",
    "- **Step 3**: 计算负载均衡损失\n",
    "\n",
    "   $$uxiliary Loss = \\alpha * CV$$\n",
    "\n",
    "   其中 $\\alpha$ 是缩放系数."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dffcd0-7c9e-40bf-9c91-1636aa571d46",
   "metadata": {},
   "source": [
    "## MoE Gate\n",
    "\n",
    "MoE 门控单元决定每次前向传播激活的 Experts 模块及其权重，同时计算 MoE 辅助损失."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cba5532c-e8d2-40ff-9ed6-b8b33b4a31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEGate(nn.Module):\n",
    "    def __init__(self, config: LMConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "        self.n_routed_experts = config.n_routed_experts # 总专家数量\n",
    "\n",
    "        self.scoring_func = config.scoring_func # 评分函数\n",
    "        self.alpha = config.aux_loss_alpha # 辅助损失的 alpha 参数\n",
    "        self.seq_aux = config.seq_aux # 是否启用序列辅助损失\n",
    "\n",
    "        self.norm_topk_prob = config.norm_topk_prob\n",
    "        self.gating_dim = config.dim\n",
    "        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim))) # 混合专家层 -> embeding dim 层\n",
    "        self.reset_parameter()\n",
    "\n",
    "    def reset_parameter(self):\n",
    "        import torch.nn.init as init\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        bsz, seq_len, h = hidden_states.shape\n",
    "        hidden_states = hidden_states.view(-1, h) # (total_tokens, hidden_dim) 其中 total_tokens = bsz * seq_len\n",
    "        logits = F.linear(hidden_states, self.weight, None) # logits = hidden_states · self.weight.T\n",
    "        # print(f'shape of logits computed: {logits.shape}') # (total_tokens, n_routed_experts)\n",
    "\n",
    "        if self.scoring_func == 'softmax':\n",
    "            scores = logits.softmax(dim=-1)\n",
    "        else:\n",
    "            raise NotImplementedError(f'insupportable scoring function for MoE gating: {self.scoring_func}')\n",
    "\n",
    "        topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)\n",
    "        if self.top_k > 1 and self.norm_topk_prob: # prob 归一化\n",
    "            denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e20\n",
    "            topk_weight = topk_weight / denominator\n",
    "\n",
    "        if self.training and self.alpha > 0.0:\n",
    "            scores_for_aux = scores\n",
    "            aux_topk = self.top_k\n",
    "            topk_idx_for_aux_loss = topk_idx.view(bsz, -1) # (bsz, seq_len * num_experts_per_tok)\n",
    "            if self.seq_aux:\n",
    "                ################# 对所有批次计算 Experts 重要性得分 #################\n",
    "                scores_for_seq_aux = scores_for_aux.view(bsz, seq_len, -1)\n",
    "                ce = torch.zeros(bsz, self.n_routed_experts, device=hidden_states.device)\n",
    "                # 根据索引将 torch.ones 累加到 ce 中，并执行归一化\n",
    "                ce.scatter_add_(1, topk_idx_for_aux_loss,\n",
    "                                torch.ones(bsz, seq_len * aux_topk, device=hidden_states.device)).div_(\n",
    "                    seq_len * aux_topk / self.n_routed_experts)\n",
    "                ######################### 计算负载均衡损失 ##########################\n",
    "                aux_loss = (ce * scores_for_seq_aux.mean(dim=1)).sum(dim=1).mean() * self.alpha\n",
    "            else:\n",
    "                mask_ce = F.one_hot(topk_idx_for_aux_loss.view(-1), num_classes=self.n_routed_experts)\n",
    "                ce = mask_ce.float().mean(0)\n",
    "                Pi = scores_for_aux.mean(0)\n",
    "                fi = ce * self.n_routed_experts\n",
    "                aux_loss = (Pi * fi).sum() * self.alpha\n",
    "        else:\n",
    "            aux_loss = 0\n",
    "        return topk_idx, topk_weight, aux_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f5c52-aad9-4de8-a571-d47de6f0d7d3",
   "metadata": {},
   "source": [
    "接下来，我们设置一个 batch size = 4, seq len =16, emb dim = 512 的输入向量，在 MoE 门控单元前向传播进行观察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b3acf9e-b814-49f7-954f-246c33daa23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新创建一个 LMConfig 使得 use MoE 为 True\n",
    "LMConfig_MoE = LMConfig(n_layers=2, use_moe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "534e4ffa-2a4d-4a0d-8906-997f23ec5098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: topk_idx = torch.Size([64, 2]), topk_weight = torch.Size([64, 2]), aux_loss = 0.10163114219903946\n",
      "token 0 选择的专家：idx = tensor([2, 0]), weight = tensor([5.8955e-21, 1.9281e-21], grad_fn=<SelectBackward0>)\n",
      "辅助损失：aux_loss = 0.10163114219903946\n"
     ]
    }
   ],
   "source": [
    "gate = MoEGate(LMConfig_MoE)\n",
    "hidden_states = torch.randn((4, 16, 512))\n",
    "topk_idx, topk_weight, aux_loss = gate(hidden_states)\n",
    "print(f'shape: topk_idx = {topk_idx.shape}, topk_weight = {topk_weight.shape}, aux_loss = {aux_loss}')\n",
    "print(f'token 0 选择的专家：idx = {topk_idx[0]}, weight = {topk_weight[0]}')\n",
    "print(f'辅助损失：aux_loss = {aux_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "521d65d2-bbcb-4b5c-8e1e-d532ae79b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "del gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3773961-8e78-4bd1-91af-8d8384bd31d3",
   "metadata": {},
   "source": [
    "### MoE Feed Forward NetWork\n",
    "\n",
    "完成 MoE 门控单元的设计后，我们可以对 MoE 前向传播网络进行重新设计."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31c28178-7faf-49aa-a813-5105129eefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEFeedForward(nn.Module):\n",
    "    def __init__(self, config: LMConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # 可选专家层\n",
    "        self.experts = nn.ModuleList([\n",
    "            FeedForward(config)\n",
    "            for _ in range(config.n_routed_experts)\n",
    "        ])\n",
    "        self.gate = MoEGate(config)\n",
    "        # 共享专家层d\n",
    "        if config.n_shared_experts is not None:\n",
    "            self.shared_experts = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        orig_shape = x.shape\n",
    "        bsz, seq_len, _ = x.shape\n",
    "        # 使用门控机制选择专家\n",
    "        topk_idx, topk_weight, aux_loss = self.gate(x)\n",
    "        x = x.view(-1, x.shape[-1])\n",
    "        flat_topk_idx = topk_idx.view(-1)\n",
    "        if self.training:\n",
    "            # 训练模式下，重复输入数据\n",
    "            x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0)\n",
    "            y = torch.empty_like(x, dtype=torch.float16)\n",
    "            # 将输入数据中选择第 i 个专家的部分输入到该专家网络中进行处理，并将输出结果存储到 y 中对应位置的元素\n",
    "            for i, expert in enumerate(self.experts):\n",
    "                y[flat_topk_idx == i] = expert(x[flat_topk_idx == i]).to(y.dtype)  #  确保类型一致\n",
    "            y = (y.view(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)).sum(dim=1) # 加权求和\n",
    "            y = y.view(*orig_shape) # 恢复原始形状\n",
    "        else:\n",
    "            # 推理模式下，只选择最优专家\n",
    "            y = self.moe_infer(x, flat_topk_idx, topk_weight.view(-1, 1)).view(*orig_shape)\n",
    "        if self.config.n_shared_experts is not None:\n",
    "            y = y + self.shared_experts(identity)\n",
    "        self.aux_loss = aux_loss\n",
    "        return y\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def moe_infer(self, x, flat_expert_indices, flat_expert_weights):\n",
    "        expert_cache = torch.zeros_like(x)\n",
    "        idxs = flat_expert_indices.argsort() # [exp_tok_11, exp_tok_12, exp_tok_21, exp_tok_22] -> idxs[2 * token_idxs] = moe_expert_name\n",
    "        tokens_per_expert = flat_expert_indices.bincount().cpu().numpy().cumsum(0) #  [1, 3, 1, 2] -> bincount -> [0, 2, 1, 1] -> cumsum -> [0, 2, 3, 4]\n",
    "        token_idxs = idxs // self.config.num_experts_per_tok\n",
    "        # 例如当tokens_per_expert=[6, 15, 20, 26, 33, 38, 46, 52]\n",
    "        # 当token_idxs=[3, 7, 19, 21, 24, 25,  4,  5,  6, 10, 11, 12...]\n",
    "        # 意味着当token_idxs[:6] -> [3,  7, 19, 21, 24, 25,  4]位置的token都由专家0处理，token_idxs[6:15]位置的token都由专家1处理......\n",
    "        for i, end_idx in enumerate(tokens_per_expert):\n",
    "            start_idx = 0 if i == 0 else tokens_per_expert[i - 1]\n",
    "            if start_idx == end_idx:\n",
    "                continue\n",
    "            expert = self.experts[i]\n",
    "            exp_token_idx = token_idxs[start_idx:end_idx]\n",
    "            expert_tokens = x[exp_token_idx]\n",
    "            expert_out = expert(expert_tokens).to(expert_cache.dtype)\n",
    "            expert_out.mul_(flat_expert_weights[idxs[start_idx:end_idx]]) # 专家网络输出加权\n",
    "            # 使用 scatter_add_ 进行 sum 操作\n",
    "            expert_cache.scatter_add_(0, exp_token_idx.view(-1, 1).repeat(1, x.shape[-1]), expert_out)\n",
    "\n",
    "        return expert_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f42fce-8bca-4853-8367-929c200f1b33",
   "metadata": {},
   "source": [
    "接下来，我们设置一个 batch size = 4, seq len =16, emb dim = 512 的输入向量，在 MoE 门控单元前向传播进行观察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c7fb409-be15-4ee9-8a77-edacfacb3743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出张量：shape = torch.Size([4, 16, 512]), 辅助损失：aux_loss = 0\n"
     ]
    }
   ],
   "source": [
    "moe_ffn = MoEFeedForward(LMConfig_MoE).eval()\n",
    "x = torch.randn((4, 16, 512))\n",
    "output = moe_ffn(x)\n",
    "print(f'输出张量：shape = {output.shape}, 辅助损失：aux_loss = {moe_ffn.aux_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "698e4148-8f68-4f05-a33a-a76f70e20760",
   "metadata": {},
   "outputs": [],
   "source": [
    "del moe_ffn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec4c71c-00a3-48bc-abf1-d88b6fb94444",
   "metadata": {},
   "source": [
    "之前提到过，MiniMind MoE 和 MiniMind Dense 的最大差别在于 FFN 模块的不同，我们可以对之前声明的 MiniMindBlock 进行继承，添加 MoE FFN 选项."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bad938b-bcf6-4bc5-ba2c-9c9689656959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DM stands for Dense & Moe\n",
    "class MiniMindBlock_DM(MiniMindBlock):\n",
    "    def __init__(self, layer_id: int, config: LMConfig):\n",
    "        super().__init__(layer_id, config)\n",
    "        self.feed_forward = FeedForward(config) if not config.use_moe else MoEFeedForward(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "778a8697-458a-47a4-b2bd-9bbdcaf64d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类变量属性检查：layer_id = 1, 类函数属性检查：forward func = <bound method MiniMindBlock.forward of MiniMindBlock_DM(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (wk): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (wv): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (wo): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      "  (feed_forward): MoEFeedForward(\n",
      "    (experts): ModuleList(\n",
      "      (0-3): 4 x FeedForward(\n",
      "        (w1): Linear(in_features=512, out_features=1408, bias=False)\n",
      "        (w2): Linear(in_features=1408, out_features=512, bias=False)\n",
      "        (w3): Linear(in_features=512, out_features=1408, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (gate): MoEGate()\n",
      "    (shared_experts): FeedForward(\n",
      "      (w1): Linear(in_features=512, out_features=1408, bias=False)\n",
      "      (w2): Linear(in_features=1408, out_features=512, bias=False)\n",
      "      (w3): Linear(in_features=512, out_features=1408, bias=False)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 检查继承是否正常\n",
    "\n",
    "miniblock_dm = MiniMindBlock_DM(1, LMConfig_MoE)\n",
    "print(f'类变量属性检查：layer_id = {miniblock_dm.layer_id}, 类函数属性检查：forward func = {miniblock_dm.forward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c99c0-ab12-49e3-805a-493de3cd3613",
   "metadata": {},
   "source": [
    "我们仍然设置一个 batch size = 4, seq len =16, emb dim = 512 的输入向量，在 MoE 门控单元前向传播进行观察."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f97a54d-aa1a-45f2-ba86-4e31f17f9cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出张量：shape = torch.Size([4, 16, 512]), KV Cache: shape Key = torch.Size([4, 16, 2, 64]), shape Value = torch.Size([4, 16, 2, 64])\n",
      "辅助损失：aux_loss = 0\n"
     ]
    }
   ],
   "source": [
    "miniblock_dm.eval() # 冻结参数，直接 moe infer\n",
    "x = torch.randn((4, 16, 512))\n",
    "pos_cis = precompute_pos_cis(64, 16)\n",
    "output, past_kv = miniblock_dm(x, pos_cis, use_cache=True)\n",
    "print(f'输出张量：shape = {output.shape}, KV Cache: shape Key = {past_kv[0].shape}, shape Value = {past_kv[1].shape}')\n",
    "print(f'辅助损失：aux_loss = {miniblock_dm.feed_forward.aux_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "925c973b-44d0-4774-a515-404f76a43a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del miniblock_dm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e64f09-ab3f-42a5-ac89-3809c1b99c26",
   "metadata": {},
   "source": [
    "如此，我们便完成了包含 MoE 和 Dense 两种 FFN 选项的 MiniMind Block 的定义，我们对此前声明的 MiniMindLM 作继承修改，使其具备 MoE 架构."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cec79087-53e4-4925-95e2-0c46ad28aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniMindLM_DM(MiniMindLM):\n",
    "    def __init__(self, params: LMConfig = None):\n",
    "        super().__init__(params)\n",
    "        structure = 'MoE' if params.use_moe else 'Dense'\n",
    "        print(f'Initializing MiniMind {structure} Model...\\n')\n",
    "        self.layers = nn.ModuleList([MiniMindBlock_DM(l, params) for l in range(self.n_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc4517ca-b5ea-4756-9c51-7da415140e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MiniMind Dense Model...\n",
      "\n",
      "Initializing MiniMind MoE Model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 检查一下\n",
    "a = MiniMindLM_DM(LMConfig_Dense)\n",
    "b = MiniMindLM_DM(LMConfig_MoE)\n",
    "del a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a586a43-6f9b-4766-8d5d-24980d1a7241",
   "metadata": {},
   "source": [
    "接下来，我们设置一条长度为 4 的 token 序列，使用 MiniMindLM 对其执行一次前向传播，观察传播过程与返回值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5009f2b-600c-4c30-b57a-85757301e6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MiniMind MoE Model...\n",
      "\n",
      "====> forward propagation started, num_minimind_blocks = 2\n",
      "------------> entering minimind block: id = 0\n",
      "<------------ finished, size_cache_k = torch.Size([1, 4, 2, 64]), size_cache_v = torch.Size([1, 4, 2, 64])\n",
      "------------> entering minimind block: id = 1\n",
      "<------------ finished, size_cache_k = torch.Size([1, 4, 2, 64]), size_cache_v = torch.Size([1, 4, 2, 64])\n",
      "<==== forward propagation completed, num_kv_cache = 2\n",
      "\n",
      "返回 logits：size = torch.Size([1, 4, 6400]), 返回 aux_loss: 0,  返回 KV Cache List： len = 2\n"
     ]
    }
   ],
   "source": [
    "MiniMind_MoE = MiniMindLM_DM(LMConfig_MoE)\n",
    "input_ids = torch.Tensor([1,  3,  5,  7]).long().reshape(1,  4)\n",
    "OUT = MiniMind_MoE(input_ids,  use_cache=True)\n",
    "print(f'返回 logits：size = {OUT.logits.shape}, 返回 aux_loss: {OUT.aux_loss},  返回 KV Cache List： len = {len(OUT.past_key_value)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0fe2fa3-f377-45da-aee1-b8f46341a470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gernerating new token: idx = 4\n",
      "====> forward propagation started, num_minimind_blocks = 2\n",
      "------------> entering minimind block: id = 0\n",
      "<------------ finished, size_cache_k = torch.Size([1, 4, 2, 64]), size_cache_v = torch.Size([1, 4, 2, 64])\n",
      "------------> entering minimind block: id = 1\n",
      "<------------ finished, size_cache_k = torch.Size([1, 4, 2, 64]), size_cache_v = torch.Size([1, 4, 2, 64])\n",
      "<==== forward propagation completed, num_kv_cache = 2\n",
      "\n",
      "gernerating new token: idx = 5\n",
      "====> forward propagation started, num_minimind_blocks = 2\n",
      "------------> entering minimind block: id = 0\n",
      "<------------ finished, size_cache_k = torch.Size([1, 1, 2, 64]), size_cache_v = torch.Size([1, 1, 2, 64])\n",
      "------------> entering minimind block: id = 1\n",
      "<------------ finished, size_cache_k = torch.Size([1, 1, 2, 64]), size_cache_v = torch.Size([1, 1, 2, 64])\n",
      "<==== forward propagation completed, num_kv_cache = 2\n",
      "\n",
      "gernerating new token: idx = 6\n",
      "====> forward propagation started, num_minimind_blocks = 2\n",
      "------------> entering minimind block: id = 0\n",
      "<------------ finished, size_cache_k = torch.Size([1, 1, 2, 64]), size_cache_v = torch.Size([1, 1, 2, 64])\n",
      "------------> entering minimind block: id = 1\n",
      "<------------ finished, size_cache_k = torch.Size([1, 1, 2, 64]), size_cache_v = torch.Size([1, 1, 2, 64])\n",
      "<==== forward propagation completed, num_kv_cache = 2\n",
      "\n",
      "new tokens list :[tensor([[4065]]), tensor([[431]]), tensor([[2541]])]\n",
      "\n",
      "生成结果：tensor([[   1,    3,    5,    7, 4065,  431, 2541]])\n"
     ]
    }
   ],
   "source": [
    "# 我们让 MiniMind 根据我们设计的 四个输入 token 生成输出\n",
    "out = MiniMind_MoE.generate(input_ids,  max_new_tokens=8,  use_cache=True)\n",
    "print(f'生成结果：{out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cdccdac-930e-4670-8605-2963d0e62d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del MiniMind_MoE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f2172-f021-442c-ae02-611a30506ebd",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "\n",
    "- [十分钟读懂旋转编码（RoPE）](https://www.zhihu.com/tardis/zm/art/647109286?source_id=1003)\n",
    "- [混合专家模型 MoE 的全面指南](https://blog.csdn.net/Code1994/article/details/145209710#:~:text=%E4%B8%BA%E4%BA%86%E5%9C%A8%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%AE%9E%E7%8E%B0%E4%B8%93%E5%AE%B6%E7%9A%84%E6%9B%B4%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83%EF%BC%8C%E8%BE%85%E5%8A%A9%E6%8D%9F%E5%A4%B1%EF%BC%88%E4%B9%9F%E7%A7%B0%E4%B8%BA%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%8D%9F%E5%A4%B1%EF%BC%89%E8%A2%AB%E6%B7%BB%E5%8A%A0%E5%88%B0%E4%BA%86%E7%BD%91%E7%BB%9C%E7%9A%84%E5%B8%B8%E8%A7%84%E6%8D%9F%E5%A4%B1%E4%B8%AD%E3%80%82%20%E5%AE%83%E5%A2%9E%E5%8A%A0%E4%BA%86%E4%B8%80%E4%B8%AA%E7%BA%A6%E6%9D%9F%EF%BC%8C%E8%BF%AB%E4%BD%BF%E4%B8%93%E5%AE%B6%E5%85%B7%E6%9C%89%E7%9B%B8%E7%AD%89%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E3%80%82,%E8%BF%99%E4%B8%AA%E8%BE%85%E5%8A%A9%E6%8D%9F%E5%A4%B1%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86%E6%98%AF%E5%9C%A8%E6%95%B4%E4%B8%AA%E6%89%B9%E6%AC%A1%E4%B8%AD%E5%AF%B9%E6%AF%8F%E4%B8%AA%E4%B8%93%E5%AE%B6%E7%9A%84%E8%B7%AF%E7%94%B1%E5%99%A8%E5%80%BC%E8%BF%9B%E8%A1%8C%E6%B1%82%E5%92%8C%EF%BC%9A%20%E8%BF%99%E4%B8%BA%E6%88%91%E4%BB%AC%E6%8F%90%E4%BE%9B%E4%BA%86%E6%AF%8F%E4%B8%AA%E4%B8%93%E5%AE%B6%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E5%88%86%E6%95%B0%EF%BC%8C%E8%BF%99%E4%BA%9B%E5%88%86%E6%95%B0%E8%A1%A8%E7%A4%BA%E6%97%A0%E8%AE%BA%E8%BE%93%E5%85%A5%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%8C%E7%BB%99%E5%AE%9A%E4%B8%93%E5%AE%B6%E8%A2%AB%E9%80%89%E4%B8%AD%E7%9A%84%E5%8F%AF%E8%83%BD%E6%80%A7%E3%80%82%20%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E5%88%A9%E7%94%A8%E8%BF%99%E4%BA%9B%E5%88%86%E6%95%B0%E6%9D%A5%E8%AE%A1%E7%AE%97%E5%8F%98%E5%BC%82%E7%B3%BB%E6%95%B0%EF%BC%88CV%EF%BC%89%EF%BC%8C%E5%AE%83%E5%91%8A%E8%AF%89%E6%88%91%E4%BB%AC%E4%B8%93%E5%AE%B6%E4%B9%8B%E9%97%B4%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E5%88%86%E6%95%B0%E7%9A%84%E5%B7%AE%E5%BC%82%E7%A8%8B%E5%BA%A6%E3%80%82)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
